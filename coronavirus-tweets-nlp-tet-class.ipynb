{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1472453,"sourceType":"datasetVersion","datasetId":863934}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ndf_test=pd.read_csv('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv',encoding=\"latin-1\")\ndf_train=pd.read_csv('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv',encoding=\"latin-1\")\ndf_test=df_test.drop(['UserName','ScreenName','Location'],axis=1)\ndf_train=df_train.drop(['UserName','ScreenName','Location'],axis=1)\ndf_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T18:28:29.834568Z","iopub.execute_input":"2025-08-13T18:28:29.834887Z","iopub.status.idle":"2025-08-13T18:28:30.043881Z","shell.execute_reply.started":"2025-08-13T18:28:29.834867Z","shell.execute_reply":"2025-08-13T18:28:30.042979Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   UserName  ScreenName             Location     TweetAt  \\\n0         1       44953                  NYC  02-03-2020   \n1         2       44954          Seattle, WA  02-03-2020   \n2         3       44955                  NaN  02-03-2020   \n3         4       44956          Chicagoland  02-03-2020   \n4         5       44957  Melbourne, Victoria  03-03-2020   \n\n                                       OriginalTweet           Sentiment  \n0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative  \n1  When I couldn't find hand sanitizer at Fred Me...            Positive  \n2  Find out how you can protect yourself and love...  Extremely Positive  \n3  #Panic buying hits #NewYork City as anxious sh...            Negative  \n4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>44953</td>\n      <td>NYC</td>\n      <td>02-03-2020</td>\n      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n      <td>Extremely Negative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>44954</td>\n      <td>Seattle, WA</td>\n      <td>02-03-2020</td>\n      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>44955</td>\n      <td>NaN</td>\n      <td>02-03-2020</td>\n      <td>Find out how you can protect yourself and love...</td>\n      <td>Extremely Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>44956</td>\n      <td>Chicagoland</td>\n      <td>02-03-2020</td>\n      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>44957</td>\n      <td>Melbourne, Victoria</td>\n      <td>03-03-2020</td>\n      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n      <td>Neutral</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\n\ndf_test['Sentiment'] = le.fit_transform(df_test['Sentiment'])\ndf_train['Sentiment'] = le.fit_transform(df_train['Sentiment'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T18:28:30.054520Z","iopub.execute_input":"2025-08-13T18:28:30.054788Z","iopub.status.idle":"2025-08-13T18:28:30.634531Z","shell.execute_reply.started":"2025-08-13T18:28:30.054761Z","shell.execute_reply":"2025-08-13T18:28:30.633718Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#start splitting:\nfrom sklearn.model_selection import train_test_split \nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntrain_df, val_df = train_test_split(df_train, test_size=0.2, random_state=42)\ntokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-tiny')\nnum_label = df_train['Sentiment'].nunique()\nmodel = AutoModelForSequenceClassification.from_pretrained('prajjwal1/bert-tiny', num_labels=num_label)\n\ntrain_token = tokenizer(train_df['OriginalTweet'].tolist(), truncation=True, padding='max_length', max_length=128)\nval_token = tokenizer(val_df['OriginalTweet'].tolist(), truncation=True, padding='max_length', max_length=128)\ntest_token = tokenizer(df_test['OriginalTweet'].tolist(), truncation=True, padding='max_length', max_length=128)\n\ntrain_labels = train_df['Sentiment'].tolist()\nval_labels = val_df['Sentiment'].tolist()\ntest_labels = df_test['Sentiment'].tolist() \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T18:28:30.635391Z","iopub.execute_input":"2025-08-13T18:28:30.635802Z","iopub.status.idle":"2025-08-13T18:28:44.610653Z","shell.execute_reply.started":"2025-08-13T18:28:30.635775Z","shell.execute_reply":"2025-08-13T18:28:44.609910Z"}},"outputs":[{"name":"stderr","text":"2025-08-13 18:28:36.922206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755109716.950515     222 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755109716.959178     222 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch \nfrom torch.utils.data import Dataset,DataLoader\n\nclass SimpleDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\ntrain_dataset = SimpleDataset(train_token, train_labels)\nval_dataset   = SimpleDataset(val_token, val_labels)\ntest_dataset  = SimpleDataset(test_token, test_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=16)\ntest_loader  = DataLoader(test_dataset, batch_size=16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T18:28:44.611452Z","iopub.execute_input":"2025-08-13T18:28:44.611984Z","iopub.status.idle":"2025-08-13T18:28:44.618813Z","shell.execute_reply.started":"2025-08-13T18:28:44.611963Z","shell.execute_reply":"2025-08-13T18:28:44.617964Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T18:29:38.385497Z","iopub.execute_input":"2025-08-13T18:29:38.386119Z","iopub.status.idle":"2025-08-13T18:29:38.530627Z","shell.execute_reply.started":"2025-08-13T18:29:38.386093Z","shell.execute_reply":"2025-08-13T18:29:38.529733Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-1): 2 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=128, out_features=128, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=128, out_features=5, bias=True)\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from tqdm import tqdm\n\nnum_epochs = 3  \nmodel.train()  \n\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n    loop = tqdm(train_loader, leave=True)\n    for batch in loop:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        \n        outputs = model(**batch)\n        loss = outputs.loss\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        loop.set_description(f\"Loss: {loss.item():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T18:30:00.854569Z","iopub.execute_input":"2025-08-13T18:30:00.855332Z","iopub.status.idle":"2025-08-13T18:31:15.229356Z","shell.execute_reply.started":"2025-08-13T18:30:00.855301Z","shell.execute_reply":"2025-08-13T18:31:15.228633Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Loss: 1.2298: 100%|██████████| 2058/2058 [00:26<00:00, 78.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Loss: 0.7254: 100%|██████████| 2058/2058 [00:24<00:00, 84.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Loss: 1.2935: 100%|██████████| 2058/2058 [00:23<00:00, 86.52it/s]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nimport pandas as pd\n\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        preds = torch.argmax(outputs.logits, dim=-1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(batch['labels'].cpu().numpy())\n\naccuracy = accuracy_score(all_labels, all_preds)\nprecision = precision_score(all_labels, all_preds, average='weighted')\nrecall = recall_score(all_labels, all_preds, average='weighted')\nf1 = f1_score(all_labels, all_preds, average='weighted')\n\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Test Precision: {precision:.4f}\")\nprint(f\"Test Recall: {recall:.4f}\")\nprint(f\"Test F1-score: {f1:.4f}\")\nsubmission = pd.DataFrame({\n    'OriginalTweet': df_test['OriginalTweet'],\n    'Sentiment_True': df_test['Sentiment'],\n    'Sentiment_Pred': all_preds\n})\n\nsubmission.to_csv('test_predictions.csv', index=False)\nprint(\"CSV file 'test_predictions.csv' saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T18:35:32.044790Z","iopub.execute_input":"2025-08-13T18:35:32.045616Z","iopub.status.idle":"2025-08-13T18:35:33.031646Z","shell.execute_reply.started":"2025-08-13T18:35:32.045588Z","shell.execute_reply":"2025-08-13T18:35:33.030977Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 0.6685\nTest Precision: 0.6771\nTest Recall: 0.6685\nTest F1-score: 0.6700\nCSV file 'test_predictions.csv' saved successfully!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"","metadata":{}}]}